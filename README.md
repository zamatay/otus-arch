# otus-arch

### Для запуска проекта необходимо установить утилиту goose go install github.com/pressly/goose/v3/cmd/goose@latest

### Далее выполнить команду make run, что создаст БД и запустить все компоненты

### Для применения миграций выполнить команду run up

## ДЗ2

## SQL Скрипты
### Создание индекса
CREATE INDEX idx_first_last_name
ON public.users USING btree (first_name, last_name)
include (id, login, birthday, gender_id, city, enabled, interests);

### Анализирование запроса
explain analyse
select id, login, first_name, last_name, birthday, gender_id, city, enabled, interests
from users
where first_name like 'xxxxx%'  and last_name like 'xxxxx%'  
limit 100;

### Порядок нагрузки. 
В течении 10 секунд набирал указанное количество потоков и далее 1 минуту, производилась 
нагрузка с рандомным запросом из 5 символов

### Результаты
|                      |  Sample  |   Average |    Median |    90%   |     95%    |    99%  |  throughput  |
|----------------------|----------|-----------|-----------|----------|------------|---------|--------------|
| 1000 без индекса     |   2236   |  35514    |  45124    |  48452   |   48649    |   48787 |  20,6/sec    |
| 1000 с индексом      |   60360  |  929      |  844      |  2037    |   2339     |   4212  |  982,4/sec   |
| 100 без индекса      |   1287   |  4475     |  4932     |  5356    |   5423     |   5564  |  19,8/sec    |
| 100 с индексом       |   47562  |  76       |  18       |  208     |   238      |   309   |  1144/sec    |
| 10 без индекса       |   1209   |  460      |  415      |  797     |   854      |   974   |  20,1/sec    |
| 10 с индексом        |   56610  |  9        |  11       |  20      |   23       |   28    |  943/sec     |

## ДЗ3

### Порядок нагрузки.
В течении 10 секунд набирал указанное количество потоков и далее 1 минуту, производилась
нагрузка с рандомным запросом из 5 символов по методу Search b рандомному числу по методу Get
В кодовой базе была реализован патерн CQRS с рандомной балансировкой между репликами, после 
чего было повторно сняты показания

### Результаты
|                   | Sample | Average | Median | 90%    | 95%    | 99%    | throughput |
|-------------------|--------|---------|--------|--------|--------|--------|------------|
| 1000 без партиции | 16     | 136488  | 129326 | 153386 | 153393 | 155548 | 6,0/min    |
| 1000 с партицией  | 4057   | 11376   | 11848  | 14135  | 14760  | 15472  | 56,5/sec   |
| 100 без партиции  | 372    | 12971   | 12994  | 17358  | 18265  | 20139  | 5,0/sec    |
| 100 с партицией   | 5034   | 1082    | 1046   | 2002   | 2222   | 2566   | 82,4/sec   |
| 10 без партиции   | 444    | 1232    | 828    | 3795   | 4110   | 4478   | 7,3/sec    |
| 10 с партицией    | 5076   | 109     | 51     | 275    | 414    | 555    | 84,5/sec   |

## ДЗ4

### Лента постов от друзей.

#### Цель:
В результате выполнения ДЗ вы создадите ленту постов друзей социальной сети

В данном задании тренируются навыки:

работа с кешами;
работа с очередями;
проектирование масштабируемых архитектур.
1) Реализовать API (синхронный rest)
   - Добавлены методы /friend/add, /friend/delete
   - Добавлены методы /post/create, /post/update, /post/delete, /post/get
   - Добавлены методы /post/feed
2) Реализовать API (синхронный rest)
    - Доработан сервис migration который наполняет таблицу posts данными
3) Реализовать кэширование ленты
    - В сервис добавлена БД redis
    - В сервис добавлен брокер сообщений kafka
    - При поступлении метода /post/create, /post/update, /post/delete, происходит обновление данных в БД redis
    - Данные обновлются через броке kafka для чего поднят еще один сервис consumer который будет читать данные из kafka 
   и отправлять их в БД redis
    - При получении постов, мы сначало проверяем есть ли эти посты в redis если есть отдаем оттуда, если нет забираем из pg и сохраняем в redis


## ДЗ5

### Масштабируемая подсистема диалогов.

#### Цель:
В результате выполнения ДЗ вы создадите базовый скелет микросервиса, который будет развиваться в дальнейших ДЗ.

В данном задании тренируются навыки:

   - декомпозиции предметной области;
   - построения элементарной архитектуры проекта.

1) Реализовать функционал:
   - Отправка сообщения пользователю (метод /dialog/{user_id}/send
   - Получение диалога между двумя пользователями (метод /dialog/{user_id}/list
2) Требования
   - Обеспечить горизонтальное масштабирование хранилищ на запись с помощью шардинга.
   - Предусмотреть:
     - Возможность решардинга
     - (опционально) “Эффект Леди Гаги” (один пользователь пишет сильно больше среднего)
     - Наиболее эффективную схему.
3) Форма сдачи ДЗ
   - Предоставить ссылку на исходный код (github, gitlab, etc)
   - Предоставить докеризированное приложение, которое можно запустить при помощи docker-compose (может лежать рядом с исходным кодом) ИЛИ развернутое приложение, доступное извне ИЛИ инструкция по запуску

#### Описание сделанных работ:
   - создал новую бд для шардинга
   - сделал в этой бд таблицу dialogs
   - доработал кодовую базу что появилось соединение с базой в которой есть шарды
   - реализовал методы dialogs/send и dialogs/list
   - Определил ключ для шардирования командой SELECT create_distributed_table('dialogs', 'from_user_id'); Теперь каждый пользователь будет лежать в одном шарде и при поиске по пользователю будет обращение только в одно место
   - доработал Makefile чтобы добавить команду up для запуска приложения с шардингом, которая будет запускать скрипт select  citus_rebalance_start();
   - доработал Makefile добавил комаду reshard
   - добавил docker-compose.yml для запуска приложения с шардингом 

## ДЗ6

### Онлайн обновление ленты новостей.

#### Цель:
Разработать WebSocket сервер, при помощи которого подключенные клиенты будут сразу получать обновления постов своих друзей. Такой подход позволит сэкономить ресурсы и обновлять молниеносно ленты активных клиентов.


1) Реализовать REST API:
   - Создание поста (метод /post/create из спецификации)
2) Реализовать асинхронное API с websocket:
https://app.swaggerhub.com/apis-docs/AVPGENIUM_1/otus-highload_architect_async/1.0.0
Реализовать отправку сообщений в канал /post/feed/posted через websocket согласно спецификации
Требования: при добавлении нового поста друга подписчику websocket'а должно приходить событие о новом посте, тем самым обеспечивая обновление ленты в реальном времени
3) Реализовать отложенную материализацию ленты:
   Требования:
      - Формирование лент работает через очередь (отложено)
      - при реализации обязательно обеспечить отправку только целевым пользователям это событие (можно применить Routing Key из RabbitMQ)
4) Реализовать отложенную материализацию ленты:) (Опционально) Формирование ленты производить через постановку задачи в очередь на часть друзей, чтобы избежать "эффекта Леди Гаги"/celebrity (один пользователь пишет сильно больше среднего)
#### Описание сделанных работ:
- Реализован метод Post/Create
- При создании поста сообщение попадает в кафка с ключом пользователя, которому отправляем сообщение
- Разработан клиент на react который подключается к WebSocket серверу и получает обновления постов через websocket, так же отправляет на создание поста
- Написан консьюмер который читает из кафки отправленные сообщения и отправляет подключившимся пользователям

## ДЗ7
1) Для выполнения ДЗ понадобится In-Memory СУБД с поддержкой функциональности application server (возможность выполнять UDF над данными) - Tarantool. Redis, Apache Ignite, Hazelcast, …
   - Подключил БД redis

2) Выбрать один из модулей приложения: например диалоги
   - выбрал модуль posts
   - ключи сделаны вида posts:user_id как хеш мапа где хранится еще одна хэш мапа вида id:text
   - Так же для получения данных по id пришлось добавить вспомагательный ключ index где храниться какой под каким user_id было вставлен id

3,4,5) Провести нагрузочное тестирование этого модуля для дальнейшего сравнения

   - Результаты
   
|                  | Sample | Average | Median | 90% | 95% | 99% | throughput  |
|------------------|--------|---------|--------|-----|-----|-----|-------------|
| PostCreate       | 262    | 4       | 4      | 5   | 6   | 6   | 640,6/sec   |
| PostCreate cache | 885831 | 6       | 4      | 11  | 19  | 38  | 14787.3/sec |
| PostGet          | 815658 | 6       | 6      | 11  | 14  | 31  | 13615,4/sec |
| PostGet cache    | 804613 | 6       | 5      | 13  | 18  | 37  | 13428.8/sec |

6) Сравнить результаты нагрузочного тестирования
   - Если сравнить результаты то видим кратный прирост в методе create, что же касается метода Get то прироста не обнаружено, так как БД была проиндексирована и оптимизирована под работу с получением данных
   - Специально был выбран для нагрузочного метод Get так как логика его получения усложнилась и необходимо было проверить деградацию.

## ДЗ8
   - Разделение монолита на сервисы

### Цель:
   - В результате выполнения ДЗ вы перенесете бизнес-домен монолитного приложения в отдельный сервис.

   - В данном задании тренируются навыки:
     - декомпозиции предметной области;
     - разделение монолитного приложения;
     - работа с HTTP;
     - работа с REST API или gRPC.

### Описание/Пошаговая инструкция выполнения домашнего задания:
   - Реализовать функционал:

      - Вынести систему диалогов в отдельный сервис.

   - Требования

      - Взаимодействия монолитного сервиса и сервиса чатов реализовать на REST API или gRPC.
      - Организовать сквозное логирование запросов (x-request-id). 
      - Предусмотреть то, что не все клиенты обновляют приложение быстро и кто-то может ходить через старое API (сохранение обратной совместимости).
### Что сделано

   - Вынесено приложение в отдельную папку monolit
   - Развернуто папка dialogs и перенесена доменная область диалогов в новый сервис (который будет разворачиваться на порту 8081)
   - Так как доменная область диалога не взаимодействует по бизнес правилам с монолитом, то для тренировки навыков взаимодействия сервисов, добавил проверку health монолита при проверки health dialogs. Взаимодействие осуществляется через gateway gRPC поэтому сервисы могут общаться как по grpc так и по rest
   - Так как в монолите оставил реализацию Dialogs то работать будет как по старой схеме так и по новой. Но наверное надо предусмотреть в балансировщике при хождении к монолиту перекидывать в dialogs

## ДЗ 9
### Отказоустойчивость приложений

### Цель:
В результате выполнения ДЗ вы уменьшите число точек отказа в вашем приложении.

В данном задании тренируются навыки:

проектирование отказоустойчивых архитектур;
настройка nginx;
настройка HAProxy.

### Описание/Пошаговая инструкция выполнения домашнего задания:
Поднять несколько слейвов PostgreSQL.
Реализовать соединение со слейвами PostgreSQL через haproxy.
Поднять несколько приложений и обеспечить их балансировку через nginx.
Воспроизвести нагрузку.
Под нагрузкой с помощью "kill -9" отключить один из слейвов PostgreSQL. Убедится, что система осталась работоспособной.
Под нагрузкой с помощью "kill -9" отключить один из инстансов бэкенда. Убедится, что система осталась работоспособной.

### Что сделано
  - Разнес на разные файлы конфигурации docker-compose
  - Создал два слайва pg
  - Поднял haproxy для балансировки между слайвами в приложение поменял коннект на haproxy
  - запустил приложение docker-compose c параметром --scale=3
  - поднял nginx для балансировки между приложениями
  - Под нагрузкой из jmeter метода user/get выполнил kill -9 на один из слейвов postgres и убедился, что приложение все равно работает
  - под нагрузкой из jmeter метода user/get выполнил kill -9 на один из инстансов бэкенда и убедился, что приложение все равно работает
### Выводы
  - Получил отказоустоцчивую систему с 3 приложениями (легко масштабируется параметром -- scale) и 2 слайвами pg масштабируется сложнее, необходимо перезапускать и переопределять конфиги (дальнейшая точка роста)
